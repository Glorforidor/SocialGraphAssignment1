{
 "cells": [
  {
   "source": [
    "# Working with data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "This notebook contains all the code for extracting from Twitter. Furthermore, it contains code for creating a graph from the extracted data and finding communites from that graph.  \n",
    "Data, graphs, and communities are all saved to disk.\n",
    "\n",
    "The [explainer.ipynb][1] loads the saved data and visualises it.\n",
    "\n",
    "[1]: https://nbviewer.jupyter.org/github/Glorforidor/SocialGraphAssignments/blob/master/explainer.ipynb"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "Load in necessary libraries for extracting and working with data, graphs, and communities."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Standard libraries\n",
    "import csv\n",
    "import collections\n",
    "from functools import wraps\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "import time\n",
    "\n",
    "# Third party libraries\n",
    "import community\n",
    "import networkx as nx\n",
    "import tweepy"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "---\n",
    "\n",
    "Names of files that will contain data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames of all the data files which makes up our dataset.\n",
    "tweets_filename = \"tweets.csv\"\n",
    "id_to_screen_name_filename = \"id_to_screen_name.csv\"\n",
    "user_and_friends_filename = \"user_and_friends_ids.csv\"\n",
    "user_to_friend_filename = \"user_to_friend_screen_names.csv\"\n",
    "bios_filename = \"bios.csv\"\n",
    "sentiment_tweets_filename = \"sentiment_tweets.csv\"\n",
    "communities_filename = \"communities.csv\"\n",
    "top_5_communities_filename = \"top_5_communities.csv\"\n",
    "\n",
    "# The saved graph - it is an undirected graph.\n",
    "graph_filename = \"security_network.gml\""
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "Helper functions.\n",
    "\n",
    "Twitter API have rate limit on X request per 15 minutes.  \n",
    "The retry function is used to wrap tweepy calls and each time a RateLimitError is raised, we wait 15 minutes and retry again the call.\n",
    "\n",
    "The log function is mostly used to log any errors that is not an RateLimitError."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retry(func=None, wait=900):\n",
    "    \"\"\"retry retries the function after the wait period on a RateLimitError.\n",
    "    \n",
    "    All other errors are raised.\"\"\"\n",
    "    def decorator_retry(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except tweepy.RateLimitError:\n",
    "                    print(f\"sleeping for {wait/60}min\", flush=True)\n",
    "                    time.sleep(wait)\n",
    "                except Exception:\n",
    "                    # Raise any other error back to the caller.\n",
    "                    raise\n",
    "        return wrapper\n",
    "\n",
    "\n",
    "    if func is not None:\n",
    "        return decorator_retry(func)\n",
    "\n",
    "    return decorator_retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(msg, filename):\n",
    "    \"\"\"log logs the message to the given filename.\n",
    "    \n",
    "    It will append the message to an existing file.\"\"\"\n",
    "    with open(filename, \"a\") as f:\n",
    "        f.write(msg)"
   ]
  },
  {
   "source": [
    "## Twitter scraping"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "Setup Tweepy library to authenticate with Twitter API."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Twitter tokens from the environment. \n",
    "twitter_consumer = os.environ[\"TWITTER_CONSUMER\"]\n",
    "twitter_consumer_secret = os.environ[\"TWITTER_CONSUMER_SECRET\"]\n",
    "twitter_token = os.environ[\"TWITTER_TOKEN\"]\n",
    "twitter_token_secret = os.environ[\"TWITTER_TOKEN_SECRET\"]\n",
    "\n",
    "# Use the Twitter tokens to authenticate towards Twitter.\n",
    "auth = tweepy.OAuthHandler(twitter_consumer, twitter_consumer_secret)\n",
    "auth.set_access_token(twitter_token, twitter_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "Extracting users and tweets that contain key words related to security, such as infosec."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all known screen names so we can use them as a filter.\n",
    "if os.path.exists(tweets_filename):\n",
    "    with open(tweets_filename, newline=\"\") as twitter_file:\n",
    "        csv_reader = csv.DictReader(twitter_file)\n",
    "        known_screen_names = [row[\"screen_name\"] for row in csv_reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct the search query for the Twitter API.\n",
    "query = \"(infosec OR cve OR cybersec OR cybersecurity OR ransomware)\"  # Match any words in the query string.\n",
    "twitter_filter = \"-filter:retweets\"  # Filter out retweets.\n",
    "mininum_favorites = \"min_faves:10\"  # Only fetch tweets that have at least 10 likes.\n",
    "\n",
    "# A regex pattern to find user names in a tweet.\n",
    "pattern = re.compile(r\"@\\w+\", re.UNICODE | re.MULTILINE)\n",
    "\n",
    "\n",
    "# Append new tweets if found.\n",
    "# The Twitter API, for a standard user, a limit of retrieving tweets up to 7 days in the past.\n",
    "with  open(tweets_filename, \"a\", newline=\"\") as twitter_file:\n",
    "    csv_writer = csv.writer(twitter_file, quoting=csv.QUOTE_ALL)\n",
    "    header = [\"screen_name\", \"content\", \"mentions\"]\n",
    "    csv_writer.writerow(header)\n",
    "    # Fetch 100 pages with 100 tweets per page.\n",
    "    for public_tweets in tweepy.Cursor(api.search, q=f\"{query} {twitter_filter} {mininum_favorites}\", count=100).pages(100):\n",
    "        for tweet in public_tweets:\n",
    "            screen_name = f\"@{tweet.user.screen_name}\"\n",
    "            # Skip screen names we have seen before.\n",
    "            if screen_name in known_screen_names:\n",
    "                continue\n",
    "            # If the tweet text mentions someone extract that screenname.\n",
    "            mentions = pattern.findall(tweet.text)\n",
    "            # In the a user's tweet, there can be newlines which will mess up the csv file.\n",
    "            # Therefore, the newlines are escaped.\n",
    "            csv_writer.writerow([screen_name, tweet.text.replace(\"\\n\", \"\\\\n\"), \"|\".join(mentions)])"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "From the users extracted before, we search their profiles for all their friends' ids."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all the screen names and store them in a set to remove duplicates.\n",
    "screen_names = set()\n",
    "with open(tweets_filename, newline=\"\") as twitter_file:\n",
    "    csv_reader = csv.DictReader(twitter_file)\n",
    "    for row in csv_reader:\n",
    "        screen_names.add(row[\"screen_name\"])\n",
    "        for mention in row[\"mentions\"].split(\"|\"):\n",
    "            screen_names.add(mention)\n",
    "\n",
    "# Remove empty screen name.\n",
    "screen_names.remove(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map screen name to friend ids of a Twitter user.\n",
    "friend_by_screen_name = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(user_and_friends_filename):\n",
    "    with open(user_and_friends_filename, newline=\"\") as twitter_file:\n",
    "        csv_reader = csv.DictReader(twitter_file)\n",
    "        friend_by_screen_name = {row[\"screen_name\"]: row[\"friends_ids\"] for row in csv_reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@retry\n",
    "def friends_ids(screen_name):\n",
    "    \"\"\"friends_ids fetches all friend ids of the given screen name\"\"\"\n",
    "    return {name: api.friends_ids(screen_name, count=5000)}\n",
    "\n",
    "print(\"Extract friends ids\")\n",
    "for idx, screen_name in enumerate(screen_names):\n",
    "    if idx % 1000 == 0:\n",
    "        print(str(idx) + \" number of name processed\")\n",
    "    # If the name is already in the list then continue.\n",
    "    if screen_name in friend_by_screen_name:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        val = friends_ids(screen_name)\n",
    "    except Exception as e:\n",
    "        log(str(e), \"friends_ids.log\")\n",
    "    else:\n",
    "        friend_by_screen_name.update(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(user_and_friends_filename, \"w\", newline=\"\") as twitter_file:\n",
    "    header = [\"screen_name\", \"friends_ids\"]\n",
    "    csv_writer = csv.writer(twitter_file, quoting=csv.QUOTE_ALL)\n",
    "    csv_writer.writerow(header)\n",
    "    for screen_name, friends_ids in friend_list.items():\n",
    "        # Discard twitter profiles with over 5000 friends - no one can have that many friends!\n",
    "        if len(friends_ids) == 5000:\n",
    "            continue\n",
    "        csv_writer.writerow([screen_name, \"|\".join(str(id_) for id_ in friends_ids)])"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "With the Friend ids, extract the name assoicated with that id."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_friend_ids = list(set(id_ for ids in friend_list.values() for id_ in ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screen_name_by_id = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(id_to_screen_name_filename):\n",
    "    with open(id_to_screen_name_filename, newline=\"\") as twitter_file:\n",
    "        csv_reader = csv.DictReader(twitter_file)\n",
    "        screen_name_by_id = {row[\"id\"]: row[\"screen_name\"] for row in csv_reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id_ in screen_name_by_id:\n",
    "    try:\n",
    "        # Remove all known ids.\n",
    "        unique_friend_ids.remove(id_)\n",
    "    except ValueError:\n",
    "        # keep any id that is not in the unique_friend_ids.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry\n",
    "def lookup_users(ids):\n",
    "    \"\"\"lookup_users retrieves users that is assoicated with the given ids\"\"\"\n",
    "    return api.lookup_users(ids)\n",
    "\n",
    "print(\"Extract Users from friends ids\", flush=True)\n",
    "for i in range(100, len(unique_friend_ids), 100):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"{str(i)} number of id processed\")\n",
    "    try:\n",
    "        users = lookup_users(unique_friend_ids[i-100:i])\n",
    "    except Exception as e:\n",
    "        log(str(e), \"lookup_users.log\")\n",
    "    else:\n",
    "        screen_name_by_id.update({user.id: user.screen_name for user in users})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(id_to_screen_name_filename, \"w\", newline=\"\") as twitter_file:\n",
    "    header = [\"id\", \"screen_name\"]\n",
    "    csv_writer = csv.writer(twitter_file, quoting=csv.QUOTE_ALL)\n",
    "    csv_writer.writerow(header)\n",
    "    for id_, screen_name in screen_name_by_id.items():\n",
    "        csv_writer.writerow([id_, screen_name])"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "Create file with a user and their friends' names."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map a screen name to list of friends' screen names.\n",
    "friend_names = {screen_name: [f\"@{screen_names.get(id_)}\" for id_ in ids] for screen_name, ids in friend_list.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write screen name and the friends' screen names down.\n",
    "with open(user_to_friend_filename, \"w\", newline=\"\") as twitter_file:\n",
    "    header = [\"screen_name\", \"friend_screen_names\"]\n",
    "    csv_writer = csv.writer(twitter_file, quoting=csv.QUOTE_ALL)\n",
    "    csv_writer.writerow(header)\n",
    "    for screen_name, friend_screen_names in friend_names.items():\n",
    "        csv_writer.writerow([screen_name, \"|\".join(friend_screen_names)])"
   ]
  },
  {
   "source": [
    "## Graph creation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "Construct the graph and add edges between users that follow each other."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a undirected graph as the repreentation of the Security People Network.\n",
    "g = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(user_to_friend_filename, newline=\"\") as f:\n",
    "    csv_reader = csv.DictReader(f)\n",
    "    screen_name_to_friends = {row[\"screen_name\"]: row[\"friend_screen_names\"].split(\"|\") for row in csv_reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through screen names and friend's screen names and add an edge iff both users are friend with each other.\n",
    "# In the Twitter world that is that they both follow each.\n",
    "for screen_name, friends_list in screen_name_to_friends.items():\n",
    "    for friend in friends_list:\n",
    "        if friend in screen_name_to_friends and screen_name in screen_name_to_friends[friend]:\n",
    "            g.add_edge(screen_name, friend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the graph to disk.\n",
    "nx.write_gml(g, graph_filename)"
   ]
  },
  {
   "source": [
    "## Community creation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "From the graph, find the communities by the best partition."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lovely code is from our Assignment 2:\n",
    "# https://github.com/Glorforidor/SocialGraphAssignments/blob/master/Assignment2.ipynb\n",
    "# easily viewed here:\n",
    "# https://nbviewer.jupyter.org/github/Glorforidor/SocialGraphAssignments/blob/master/Assignment2.ipynb\n",
    "\n",
    "def communities(graph):\n",
    "    \"\"\"communities find communities in the graph and return a list of communities.\n",
    "    \n",
    "    It uses the community library to find the best partition of the graph using the Louvain method.\n",
    "    \"\"\"\n",
    "    partition = community.best_partition(graph)\n",
    "    d = collections.defaultdict(list)\n",
    "    # The community.best_partition function maps nodes to a community number, below via do the opposite.\n",
    "    for com in set(partition.values()):\n",
    "        for nodes in partition.keys():\n",
    "            if partition[nodes] == com:\n",
    "                d[com].append(nodes)\n",
    "    \n",
    "    return list(d.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "security_communities = communities(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(communities_filename, \"w\", newline=\"\") as f:\n",
    "    header = [\"community_name\", \"members\"]\n",
    "    csv_writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    for i, com in enumerate(security_communities):\n",
    "        csv_writer.writerow([i, \"|\".join(com)])"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "From the communities, take the top 5 largest communities, so we only work with a subset of the communities."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_largest_communites = sorted(security_communities, key=len, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(top_5_communities_filename, \"w\", newline=\"\") as f:\n",
    "    header = [\"community_name\", \"members\"]\n",
    "    csv_writer = csv.writer(f, quoting=csv.QUOTE_ALL)\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    for i, com in enumerate(top_5_largest_communites):\n",
    "        csv_writer.writerow([i, \"|\".join(com)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "members_by_communities = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(top_5_communities_filename, newline=\"\") as twitter_file:\n",
    "    csv_reader = csv.DictReader(twitter_file)\n",
    "    members_by_communities = {row[\"community_name\"]: row[\"members\"].split(\"|\") for row in csv_reader}"
   ]
  },
  {
   "source": [
    "## Sentiment data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---\n",
    "\n",
    "With the top 5 largest communities, extract their description (bio) and their location, to pin point which type of community they belong to and where are they mostly based."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_by_name = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(bios_filename):\n",
    "    with open(bios_filename, newline=\"\") as f:\n",
    "        csv_reader = csv.DictReader(f)\n",
    "        bio_by_name = {row[\"screen_name\"]: (row[\"bio\"], row[\"location\"]) for row in csv_reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry\n",
    "def get_user(member):\n",
    "    \"\"\"get_user fetches a Twitter user.\n",
    "    \n",
    "    member: id, user_id or screen_name.\n",
    "    \"\"\"\n",
    "    return api.get_user(member)\n",
    "\n",
    "for members in members_by_communities.values():\n",
    "    for member in members:\n",
    "        if member in bio_by_name:\n",
    "            continue\n",
    "        try:\n",
    "            user = get_user(member)\n",
    "        except tweepy.TweepError as e:\n",
    "            print(f\"This member: {member} caused an error! Shame on thee {e}\")\n",
    "        else:\n",
    "            bio_by_name[member] = (user.description, user.location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bios_filename, \"w\", newline=\"\") as twitter_file:\n",
    "    header = [\"screen_name\", \"bio\", \"location\"]\n",
    "    csv_writer = csv.writer(twitter_file, quoting=csv.QUOTE_ALL)\n",
    "    csv_writer.writerow(header)\n",
    "    for screen_name, (bio, location) in bio_by_name.items():\n",
    "        # In the a user's bio, there can be newlines which will mess up the csv file.\n",
    "        # Therefore, the newlines are escaped.\n",
    "        csv_writer.writerow([screen_name, bio.replace(\"\\n\", \"\\\\n\"), location])"
   ]
  },
  {
   "source": [
    "---\n",
    "\n",
    "Extract recent tweets from community members, which will then be used to calculate some sentiment values."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_by_screen_name = collections.defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sentiment_tweets_filename, newline=\"\") as twitter_file:\n",
    "    csv_reader = csv.DictReader(twitter_file)\n",
    "    for row in csv_reader:\n",
    "        tweets_by_screen_name[row[\"screen_name\"]].append(row[\"tweets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@retry\n",
    "def get_user_timeline(member):\n",
    "    \"\"\"get_user_timeline fetches a Twitter user's timeline.\n",
    "    \n",
    "    member: id, user_id or screen_name.\n",
    "    \"\"\"\n",
    "    return api.user_timeline(member)\n",
    "\n",
    "for members in members_by_communities.values():\n",
    "    for member in members:\n",
    "        if member in tweets_by_screen_name:\n",
    "            continue\n",
    "        try:\n",
    "            statuses = get_user_timeline(member)\n",
    "        except tweepy.TweepError as e:\n",
    "            print(f\"This member: {member} caused an error! Shame on thee {e}\")\n",
    "        else:\n",
    "            tweets_by_screen_name[member] = [status.text for status in statuses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sentiment_tweets_filename, \"w\", newline=\"\") as twitter_file:\n",
    "    header = [\"screen_name\", \"tweets\"]\n",
    "    csv_writer = csv.writer(twitter_file, quoting=csv.QUOTE_ALL)\n",
    "    csv_writer.writerow(header)\n",
    "    for screen_name, tweets in tweets_by_screen_name.items():\n",
    "        for tweet in tweets:\n",
    "            # In the a user's tweet, there can be newlines which will mess up the csv file.\n",
    "            # Therefore, the newlines are escaped.\n",
    "            csv_writer.writerow([screen_name, tweet.replace(\"\\n\", \"\\\\n\")])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}